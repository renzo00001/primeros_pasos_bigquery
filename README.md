# üìä Mini Proyecto ETL con Python + BigQuery

Este repositorio contiene un **pipeline ETL (Extract, Transform, Load)** desarrollado en Python para cargar archivos Excel almacenados en una carpeta (SharePoint sincronizado) hacia una **tabla staging en BigQuery**, aplicando validaciones, limpieza y transformaciones de datos.

El objetivo principal es **automatizar la carga diaria de reportes**, evitar duplicados y garantizar que los datos lleguen a BigQuery con un esquema consistente.

---

## üèóÔ∏è Arquitectura del flujo

1. **Fuente**: Archivos Excel `.xlsb` almacenados en SharePoint (sincronizado localmente)
2. **Extract**: Lectura autom√°tica con Pandas
3. **Transform**: Limpieza, estandarizaci√≥n y casting de datos
4. **Validation**: Consulta previa a BigQuery para evitar duplicados
5. **Load**: Carga incremental (`WRITE_APPEND`) a tabla staging en BigQuery

---

## üß∞ Tecnolog√≠as utilizadas

* **Python 3**
* **Pandas / NumPy** ‚Üí transformaci√≥n de datos
* **google-cloud-bigquery** ‚Üí consultas y cargas
* **BigQuery** ‚Üí Data Warehouse
* **Regex** ‚Üí extracci√≥n de fecha desde el nombre del archivo

---

## üîê Configuraci√≥n de credenciales

BigQuery utiliza autenticaci√≥n mediante una **Service Account**.

```python
credenciales_bigquery = r'C:\ruta\a\credenciales.json'
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credenciales_bigquery
```

> ‚ö†Ô∏è **Buena pr√°ctica**: No subir el archivo de credenciales a GitHub. Agregar la ruta al `.gitignore`.

---

## üì• Funci√≥n: Realizar_Consulta

Ejecuta consultas SQL en BigQuery y devuelve el resultado como un DataFrame.

```python
def Realizar_Consulta(sql_query):
    """
    Ejecuta una consulta SQL en BigQuery y retorna el resultado como DataFrame.
    Se utiliza principalmente para validar si una fecha ya fue cargada.
    """
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credenciales_bigquery
    cliente = bigquery.Client()

    query_job = cliente.query(sql_query)
    resultado = [dict(fila) for fila in query_job.result()]

    return pd.DataFrame(resultado)
```

### ¬øPor qu√© es importante?

* Permite **cargas idempotentes**
* Evita duplicar informaci√≥n por fecha

---

## üîÑ Funci√≥n: Transformar_datos

Aplica todas las transformaciones necesarias antes de enviar los datos a BigQuery.

```python
def Transformar_datos(df_Data, fecha_archivo):
    """
    Limpia y transforma el DataFrame para que sea compatible con BigQuery.
    - Conversi√≥n de fechas Excel
    - Renombrado de columnas
    - Casting de tipos
    - Limpieza de nulos
    """
    try:
        # Conversi√≥n de fechas desde formato Excel (serial date)
        df_Data['Fecha'] = pd.to_datetime(df_Data['Fecha'], unit='D', origin='1899-12-30', errors='coerce')
        df_Data['Fecha_HU'] = pd.to_datetime(df_Data['Fecha_HU'], unit='D', origin='1899-12-30', errors='coerce')
        df_Data['CreadoEl'] = pd.to_datetime(df_Data['CreadoEl'], unit='D', origin='1899-12-30', errors='coerce')
        df_Data['FECHA VENCIMIENTO'] = pd.to_datetime(df_Data['FECHA VENCIMIENTO'], unit='D', origin='1899-12-30', errors='coerce')

        # Renombrado de columnas (data governance)
        mapeo_columnas = {
            'Fecha': 'Fecha_Reporte',
            'FECHA VENCIMIENTO': 'Fecha_Vencimiento',
            'Dias': 'DiasDePermanencia',
            'Dias Redireccion': 'DiasRedireccion',
            'CodGA': 'CodigoArticulo',
            'GrAr': 'NombreArticulo',
            'DescMaterial': 'NombreMaterial',
            'PUnit': 'PrecioUnidad',
            'CodProveedor': 'RUC_Proveedor',
            'Stock Tda Unidades': 'Stock_Tda_Unidades',
            'Cob Tda': 'Cob_Tda',
            'cob <=5': 'cob_menor_o_igual_a_5',
            'Tipo tda': 'TipoTienda',
            'Ce.Origen': 'CentroOrigen',
            'Ce': 'CentroDistribucion',
            'Status HU': 'StatusHU',
            'Gerencia Paleta': 'GerenciaPaleta',
            'Due√±o Paleta': 'DunioPaleta',
            'VentaPromedio 30D': 'VentaPromedio30D',
            'Grp Abast.': 'Grupo_Abastecimiento',
            'Alerta Paleta': 'AlertaPaleta'
        }

        df_Data.rename(columns=mapeo_columnas, inplace=True)

        # Columna de control
        df_Data['Envio'] = 3

        # C√°lculo de grupo de compra desde el c√≥digo de art√≠culo
        codigos_articulo = df_Data['CodigoArticulo'].astype(str)
        df_Data['GrComp'] = pd.to_numeric(
            np.where(
                codigos_articulo.str.len() < 8,
                codigos_articulo.str[:1],
                codigos_articulo.str[:2]
            ),
            errors='coerce'
        )

        # Columnas que deben ser STRING en BigQuery
        columnas_string = ['CentroDistribucion', 'Almacen', 'StatusHU', 'Hora', 'Gestor']
        for col in columnas_string:
            df_Data[col] = df_Data[col].astype(str).replace(['nan', 'None', 'NaT'], None)

        # Eliminaci√≥n de columnas innecesarias
        df_Data.drop(columns=['Contar'], inplace=True, errors='ignore')

        return True, df_Data

    except Exception as e:
        return False, f"Error en transformaci√≥n ({fecha_archivo}): {e}"
```

### Buenas pr√°cticas aplicadas

* Conversi√≥n expl√≠cita de tipos
* Renombrado estandarizado
* Manejo de errores por archivo

---

## üöÄ Funci√≥n: Enviar_datos_por_lote

Carga el DataFrame limpio a BigQuery usando el motor de PyArrow.

```python
def Enviar_datos_por_lote(df, nombre_tabla, fecha_archivo):
    """
    Carga un DataFrame a BigQuery usando carga por lote.
    La tabla debe existir previamente.
    """
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credenciales_bigquery
    cliente = bigquery.Client()

    job_config = bigquery.LoadJobConfig(
        write_disposition="WRITE_APPEND",
        autodetect=True
    )

    try:
        job = cliente.load_table_from_dataframe(df, nombre_tabla, job_config=job_config)
        job.result()
        print(f"‚úî {len(df)} filas cargadas | Fecha: {fecha_archivo}")
    except Exception as e:
        print(f"‚úñ Error carga {fecha_archivo}: {e}")
```

---

## üîÅ Orquestaci√≥n principal

```python
for archivo in os.listdir(ruta_carpeta):
    if archivo.lower().startswith('priorizaci√≥n'):
        # Extraer fecha desde el nombre del archivo
        extraer_fecha = re.search(r'(\d{2}\.\d{2}\.\d{4})', archivo).group(1)
        fecha_nueva = pd.to_datetime(extraer_fecha, dayfirst=True).strftime('%Y-%m-%d')

        # Validar si la fecha ya existe en BigQuery
        sql = f"SELECT COUNT(*) AS conteo_filas FROM {Tabla_bigquery} WHERE Fecha_Reporte = '{fecha_nueva}'"
        df_validacion = Realizar_Consulta(sql)

        if df_validacion['conteo_filas'][0] > 0:
            continue

        # Lectura del archivo
        df_data = pd.read_excel(os.path.join(ruta_carpeta, archivo), sheet_name='DATA', engine='pyxlsb')

        # Transformaci√≥n
        ok, df_data = Transformar_datos(df_data, fecha_nueva)
        if not ok:
            continue

        # Carga a BigQuery
        Enviar_datos_por_lote(df_data, Tabla_bigquery, fecha_nueva)
```

---

## üìà Posibles mejoras futuras

* Definir **esquema expl√≠cito** en BigQuery
* Implementar **Data Quality Checks**
* Logging estructurado (Cloud Logging)
* Orquestaci√≥n con **Cloud Functions / Airflow**

---

## üéØ Prop√≥sito del Proyecto (Caso de Uso Real)

El prop√≥sito final de este mini proyecto es **integrarlo dentro del proceso de generaci√≥n del reporte de paletas**, migrando un flujo tradicional basado en archivos Excel hacia una arquitectura **centralizada y escalable en BigQuery**.

Con esta implementaci√≥n:

* üì• Los archivos operativos contin√∫an llegando en Excel (origen real del negocio)
* üîÑ El proceso ETL automatiza la limpieza y estandarizaci√≥n
* üóÑÔ∏è BigQuery act√∫a como **fuente √∫nica de la verdad (Single Source of Truth)**
* üìä M√∫ltiples usuarios pueden conectarse simult√°neamente desde **Power BI**

---

## üìä Consumo de datos en Power BI

Los datos cargados en BigQuery est√°n pensados para ser consumidos directamente desde Power BI, permitiendo:

* üîπ Conexi√≥n directa v√≠a **conector nativo de BigQuery**
* üîπ An√°lisis en **tiempo casi real** (sin depender de archivos locales)
* üîπ Dashboards compartidos para √°reas de log√≠stica, BI y operaciones
* üîπ Eliminaci√≥n de reprocesos manuales y versiones inconsistentes

Este enfoque habilita:

* Mejor toma de decisiones
* Mayor trazabilidad de la informaci√≥n
* Escalabilidad a futuro (nuevos reportes / nuevos usuarios)

---

## ‚úÖ Conclusi√≥n

Este proyecto representa un **primer paso hacia una arquitectura moderna de datos**:

* Centralizaci√≥n en BigQuery
* Automatizaci√≥n del proceso ETL
* Consumo multiusuario desde herramientas BI

Aunque es un proyecto peque√±o, refleja un escenario **realista de transformaci√≥n de reportes operativos** hacia anal√≠tica en tiempo real, siendo una base s√≥lida para evolucionar hacia soluciones m√°s avanzadas (orquestaci√≥n, calidad de datos y modelos anal√≠ticos).
